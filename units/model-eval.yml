title: Model evaluation
theme: model
needs: model-multivariate
readings: ~
updated: ~
desc: "\n``` r\nlibrary(tidyverse)\nlibrary(modelr)\n```\n\nHow do you know if a model
  is any good? How do you measure the quality of a model? We will explore these questions
  below.\n\nThe problem\n-----------\n\nLet's assume that we are trying to develop
  a model of a phenomenon that has a functional representation of the form\n\n*y* = *f*(*x*)\n\nand
  that any measurements of *y* have errors. We typically would not know *f* -- that's
  why we are developing a model -- but for our purposes we will specify *f* and some
  simulated data.\n\n``` r\n# True function\nf <- function(x) x + 50 * sin(pi / 50
  * x)\n\n# Function with measurement error\ng <- function(x) f(x) + rnorm(n = length(x),
  mean = 0, sd = 20)\n\n# Random sample of data points\nsim_data <- function(from,
  to, by) {\n  tibble(\n    x = seq(from = from, to = to, by = by),\n    y = g(x)\n
  \ )\n}\n```\n\nLet's generate a random sample that we'll use for our modeling below.\n\n```
  r\nset.seed(439)\n\ndata_1 <- sim_data(0, 100, 0.5)\n```\n\nHere's a plot of the
  true function with the data we will use to model it.\n\n``` r\ntibble(x = seq(0,
  100, 0.5), y = f(x)) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_point(data
  = data_1) +\n  labs(title = \"True function and data to model it with\")\n```\n\n![](model-eval_files/figure-markdown_github/unnamed-chunk-4-1.png)\n\nThe
  `loess()` model\n-------------------\n\nGiven the shape of our data, we will try
  to model it using the `stats::loess()` function. This function works by creating
  local models at each point, where each local model only uses data within a given
  distance from the point. The function has the parameter `span` to control which
  data points are included in the local models; the smaller the span, the fewer points
  included in the local models.\n\nLet's look at the relationship of the `span` parameter
  to the resulting model.\n\n``` r\n# Plot loess model for given span\nplot_loess
  <- function(span) {\n  data_1 %>% \n    mutate(f = f(x)) %>% \n    add_predictions(loess(y
  ~ x, data = data_1, span = span)) %>% \n    ggplot(aes(x)) +\n    geom_line(aes(y
  = f)) +\n    geom_point(aes(y = y)) +\n    geom_line(aes(y = pred), color = \"#3366FF\",
  size = 1) +\n    labs(\n      title = str_c(\"loess, span = \", span),\n      y
  = \"y\"\n    )\n}\n```\n\n``` r\nc(10, 1, 0.75, 0.1, 0.01) %>% \n  walk(~ print(plot_loess(.)))\n```\n\n![](model-eval_files/figure-markdown_github/unnamed-chunk-6-1.png)![](model-eval_files/figure-markdown_github/unnamed-chunk-6-2.png)![](model-eval_files/figure-markdown_github/unnamed-chunk-6-3.png)![](model-eval_files/figure-markdown_github/unnamed-chunk-6-4.png)![](model-eval_files/figure-markdown_github/unnamed-chunk-6-5.png)\n\nThe
  `span` parameter clearly makes a big difference in the model:\n\n-   `span = 10`
  is has too much smoothing, washing out the variablity in the model.\n-   `span =
  1` is much better, but still has a bit too much smoothing.\n-   `span = 0.75` is
  the default and does a reasonably good job.\n-   `span = 0.1` has too little smoothing.
  The added complexity of the model is now fitting the random variation.\n-   `span
  = 0.01` has gone way too far, fitting individual random points.\n\nThe models with
  `span` equal to `0.1` and `0.01` have **overfit** the data. Models that have been
  overfit will not generalize; that is, they will not work well with new data.\n\nTest
  and training errors\n------------------------\n\nLet's return to the question of
  how to measure the quality of a model. Above we could visualize the models because
  they were very simple, with only one predictor variable `x`. It is harder to visualize
  models with many predictor variables.\n\nOne idea for measuring the quality of a
  model would be to compare its predicted values with actual values in the data. Ideally,
  we want to know the difference between predicted and actual values on *new* data.
  This is called the **test error**. Unfortunately, we usually don't have new data.
  All we have is the data used to create the model. The difference between the predicted
  and actual values on the data used to train the model is called the **training error**.\n\nSince
  the expected value for the error term of `g()` is `0` for each `x`, the best possible
  model for new data would be `f()` itself. The error on `f()` itself is called the
  **Bayes error**.\n\nUsually, we don't know the underlying function `f()`, and we
  don't have new data. In our case since our data is simulated, we know `f()`, and
  we can generate new data. Below we will generate a dataset `data_50` that has 50
  samplings of the size of `data_1` to get an accurate measure of the test error.\n\nHow
  do we measure the difference between a vector of predictions and a vector of actual
  values? Two common ways include:\n\n-   Root-mean-square error (RMSE): `sqrt(mean((y
  - pred)^2, na.rm = TRUE))`\n-   Mean absolute error (MAE): `mean(abs(y - pred),
  na.rm = TRUE)`\n\nBoth measures are supported by modelr. We'll use RMSE below.\n\nLet's
  begin by generating new dataset for testing with 50 times as much data as our original
  dataset.\n\n``` r\nset.seed(886)\n\ndata_50 <- \n  1:50 %>% \n  map_dfr(~ sim_data(0,
  100, 0.5))\n```\n\nWe will now train a series of models on the original dataset
  `data_1` using a range of `span`s. For each model, we will calculate the training
  error on `data_1` and the test error on the new data `data_50`.\n\n``` r\nspans
  <- 2^seq(1, -2, -0.2)\n\nerrors <- \n  tibble(span = spans) %>% \n  mutate(\n    train_error
  =\n      map_dbl(span, ~ rmse(loess(y ~ x, data = data_1, span = .), data_1)),\n
  \   test_error =\n      map_dbl(span, ~ rmse(loess(y ~ x, data = data_1, span =
  .), data_50))\n  )\n```\n\nThe Bayes error calculated from `data_50` is:\n\n```
  r\nbayes_error <- \n  data_50 %>% \n  summarize(bayes_error = sqrt(mean((y - f(x))^2,
  na.rm = TRUE))) %>% \n  pull(bayes_error)\n```\n\nLet's plot the errors.\n\n```
  r\nbayes <- tibble(\n  span = range(errors$span),\n  type = \"bayes_error\",\n  error
  = bayes_error\n)\n\nerrors %>% \n  gather(key = type, value = error, -span) %>%
  \n  ggplot(aes(1 / span, error, color = type)) + \n  geom_line(data = bayes) +\n
  \ geom_line() +\n  geom_point() +\n  labs(\n    title = \"Errors as a function of
  increasing model complexity\",\n    color = NULL\n  )\n```\n\n![](model-eval_files/figure-markdown_github/unnamed-chunk-10-1.png)\n\nThe
  models get more complex as `span` gets smaller. We have plotted the errors as a
  function of 1 / `span`, so the larger values on the right indicate more complex
  models. The Bayes error should be 20, the standard deviation of measurement error
  in `g()`. The lowest value of test error was approximately 20.18 -- very close to
  the Bayes error. This was achieved for `span` ≈ 0.758, which is close to the default
  `span` = 0.75 used by `loess()`.\n\nNotice from the plot that as the model gets
  more and more complex, the training error continues to decline but after a point
  the test error starts to increase. This divergence means that the model overfits
  the training data for small span values.\n\nHere we were able to generate new data
  (`data_50`) to calculate the actual test error. However, we typically will only
  have the original data (`data_1`), and getting new data will not be option. Therefore,
  it is important to be able to estimate the test error well. Since complex models
  can overfit the training data, as shown above, the training error is is not a good
  estimate of the test error, and it is *not* a good idea to use the training error
  to make decisions about the best model. In the next section, we will discuss better
  ways to estimate the test error from the original data.\n\nCross-validation\n----------------\n\nThe
  key idea of cross-validation is that in the absence of new data, we can hold out
  a portion of the original data, train the model on the rest of the data, and then
  test the model the portion that was held out. modelr provides two functions for
  generating train-test pairs:\n\n-   `crossv_mc()`: Generates `n` random partitions,
  holding out a specified proportion of the data to test with.\n-   `crossv_kfold()`:
  Splits the data into `k` exclusive partitions or folds. It uses each of the `k`
  folds as a test set with the remaining `k` - 1 folds as the training set.\n\nLet's
  see how these functions work.\n\n``` r\ndf <- \n  data_1 %>% \n  crossv_kfold(k
  = 10)\n\ndf\n```\n\n    ## # A tibble: 10 x 3\n    ##    train          test           .id
  \ \n    ##    <list>         <list>         <chr>\n    ##  1 <S3: resample> <S3:
  resample> 01   \n    ##  2 <S3: resample> <S3: resample> 02   \n    ##  3 <S3: resample>
  <S3: resample> 03   \n    ##  4 <S3: resample> <S3: resample> 04   \n    ##  5 <S3:
  resample> <S3: resample> 05   \n    ##  6 <S3: resample> <S3: resample> 06   \n
  \   ##  7 <S3: resample> <S3: resample> 07   \n    ##  8 <S3: resample> <S3: resample>
  08   \n    ##  9 <S3: resample> <S3: resample> 09   \n    ## 10 <S3: resample> <S3:
  resample> 10\n\nIn this case, `crossv_kfold()` creates 10 train-test pairs in two
  list-columns `train` and `test`. These variables are lists of resample objects.\n\nLet's
  look at the resample objects for a train-test pair.\n\n``` r\nglimpse(df$train[[1]])\n```\n\n
  \   ## List of 2\n    ##  $ data:Classes 'tbl_df', 'tbl' and 'data.frame':    201
  obs. of  2 variables:\n    ##   ..$ x: num [1:201] 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5
  ...\n    ##   ..$ y: num [1:201] 22.95 36.64 -2.32 21.26 18.88 ...\n    ##  $ idx
  : int [1:180] 1 2 3 4 5 6 8 9 10 11 ...\n    ##  - attr(*, \"class\")= chr \"resample\"\n\n```
  r\nglimpse(df$test[[1]])\n```\n\n    ## List of 2\n    ##  $ data:Classes 'tbl_df',
  'tbl' and 'data.frame':    201 obs. of  2 variables:\n    ##   ..$ x: num [1:201]
  0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 ...\n    ##   ..$ y: num [1:201] 22.95 36.64 -2.32
  21.26 18.88 ...\n    ##  $ idx : int [1:21] 7 15 31 45 51 58 65 73 77 89 ...\n    ##
  \ - attr(*, \"class\")= chr \"resample\"\n\nA resample object consists of the original
  dataset together with a set of indices that indicate the subset of the original
  data to use.\n\nFrom the index variables `idx`, we can see that the train and test
  sets are disjoint and their union is the complete original dataset.\n\n``` r\nis_empty(intersect(df$train[[1]]$idx,
  df$test[[1]]$idx))\n```\n\n    ## [1] TRUE\n\n``` r\nsetequal(union(df$train[[1]]$idx,
  df$test[[1]]$idx), 1:201)\n```\n\n    ## [1] TRUE\n\nSome model functions, such
  as `lm()` can take as input resample objects. `loess()` does not, however. For models
  such as these, you can turn a resample object into the corresponding tibble using
  `as_tibble()`.\n\n``` r\nas_tibble(df$test[[1]])\n```\n\n    ## # A tibble: 21 x
  2\n    ##        x     y\n    ##    <dbl> <dbl>\n    ##  1   3   -18.3\n    ##  2
  \  7    35.0\n    ##  3  15    82.6\n    ##  4  22    75.6\n    ##  5  25    45.1\n
  \   ##  6  28.5  58.0\n    ##  7  32    96.4\n    ##  8  36    51.9\n    ##  9  38
  \   38.3\n    ## 10  44    84.9\n    ## # ... with 11 more rows\n\nResample objects
  are quite wasteful of space, since each one contains the full dataset. A new package
  [rsample](https://topepo.github.io/rsample/) is being developed to support tidy
  modeling in a much more space-efficient way.\n\nWith `crossv_mc()`, you can independently
  specify the number of train-test pairs and the proportion of the data to hold out
  to test with. With `crossv_kfold()` the parameter `k` specifies the number of train-test
  pairs and the proportion, `1 / k`, of data to hold out to test with. To get more
  pairs with `crossv_kfold()` you can simply repeat it.\n\nThe following function
  returns the RMSE for a given span, train data, and test data.\n\n``` r\nrmse_error
  <- function(span, train, test) {\n  rmse(loess(y ~ x, data = as_tibble(train), span
  = span), as_tibble(test))\n}\n```\n\nThe following function that calculates the
  errors for a given span on all of the train-test pairs in a given CV set, and then
  calculates the mean and standard deviation of the errors.\n\n``` r\nspan_error <-
  function(span, data_cv) {\n  errors <- \n    data_cv %>% \n    select(-.id) %>%
  \n    add_column(span = span) %>% \n    pmap_dbl(rmse_error)\n  \n  tibble(\n    span
  = span,\n    error_mean = mean(errors, rm = TRUE),\n    error_sd = sd(errors, na.rm
  = TRUE)\n  )\n}\n```\n\nNext, let's use `crossv_mc()` to generate 100 train-test
  pairs with test sets consisting of approximately 20% of the data. We will then calculate
  the CV error for all `span`s using these train-test pairs.\n\n``` r\nset.seed(430)\n\ndata_mc
  <- crossv_mc(data_1, n = 100, test = 0.2)\n\nerrors_mc <- \n  spans %>% \n  map_dfr(~
  span_error(span = ., data_cv = data_mc))\n\nerrors_mc %>% \n  knitr::kable()\n```\n\n|
  \      span|  error\\_mean|  error\\_sd|\n|----------:|------------:|----------:|\n|
  \ 2.0000000|     28.13057|   2.578456|\n|  1.7411011|     27.32540|   2.572414|\n|
  \ 1.5157166|     26.29811|   2.566547|\n|  1.3195079|     25.00708|   2.558990|\n|
  \ 1.1486984|     23.06875|   2.568471|\n|  1.0000000|     21.62002|   2.532316|\n|
  \ 0.8705506|     19.58527|   2.417289|\n|  0.7578583|     18.97282|   2.323062|\n|
  \ 0.6597540|     18.84846|   2.269925|\n|  0.5743492|     18.86447|   2.256472|\n|
  \ 0.5000000|     18.90930|   2.260496|\n|  0.4352753|     18.93128|   2.254325|\n|
  \ 0.3789291|     18.88556|   2.220315|\n|  0.3298770|     18.76699|   2.171678|\n|
  \ 0.2871746|     18.60700|   2.135667|\n|  0.2500000|     18.52702|   2.109205|\n\nLet's
  compare the CV estimates of the test error with the actual test error.\n\n``` r\nerrors_mc
  %>% \n  left_join(errors, by = \"span\") %>% \n  ggplot(aes(1 / span)) +\n  geom_line(aes(y
  = test_error, color = \"test_error\")) +\n  geom_point(aes(y = test_error, color
  = \"test_error\")) +\n  geom_line(aes(y = error_mean, color = \"mc_error\")) +\n
  \ geom_pointrange(\n    aes(\n      y = error_mean, \n      ymin = error_mean -
  error_sd,\n      ymax = error_mean + error_sd,\n      color = \"mc_error\"\n    )\n
  \ ) +\n  labs(\n    title = \"Cross-validation and test errors\",\n    y = \"error\",\n
  \   color = NULL\n  )\n```\n\n![](model-eval_files/figure-markdown_github/unnamed-chunk-18-1.png)\n\nFrom
  this plot, we can see that CV error estimates from `crossv_mc()` underestimate the
  true test error in the range with 1 / `span` &gt;= 1. The results with `crossv_kfold()`
  are similar. The line ranges reflect one standard error on either side of the mean
  error and show that there is considerable uncertainty in the error estimates. Except
  for the largest 1 / `span` = 4, the test error is within one standard error of the
  mean.\n\nTypically we will only know the CV errors. Here's the rule of thumb for
  choosing a tuning parameter knowing only the CV errors:\n\n-   Start with the parameter
  that has the lowest mean CV error. In this case, that would be `span` = 0.25.\n-
  \  Imagine first horizontally sliding the one-standard-error range for `span` =
  0.25 all the way to the left of the plot, and then sliding it to the right, stoping
  when the first CV mean error is within this range. In our case, the top of the one-standard-error
  range for `span` = 0.25 is approximately 20.6. The CV mean error for `span` = 1
  is larger than this, but the next most complex model with `span` ≈ 0.871 is smaller,
  so we would choose this `span`.\n\nWe saw above that if we knew the test error,
  we would choose `span` ≈ 0.758 as the optimal parameter. But we usually don't know
  the test error, and in this case the test errors for 0.871 and 0.758 were very close,
  with both quite close to the Bayes error of 20.\n\n``` r\nerrors %>% \n  filter(span
  < 0.9, span > 0.7) %>% \n  select(span, test_error) %>% \n  knitr::kable()\n```\n\n|
  \      span|  test\\_error|\n|----------:|------------:|\n|  0.8705506|      20.4014|\n|
  \ 0.7578583|      20.1787|\n"
